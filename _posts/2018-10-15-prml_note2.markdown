---
layout:     post
title:      "PRML学习笔记（二）"
subtitle:   "第二章 概率分布"
date:       2018-10-15 00:15:18
author:     "Pelhans"
header-img: "img/prml.jpg"
header-mask: 0.3 
catalog:    true
tags:
    - PRML
---


> PRML 和 ESL 的学习基本上是学十得一。稳扎稳打再来一次

* TOC
{:toc}

# 概率分布

## 2.1 二元变量
### 2.1.0.1 伯努利分布

在给定有限次观测数据的前提下，对随机变量x的概率分布p(x)进行建模。这个问题被称为密度估计。分布函数是直接和概率测度相关的，而密度函数需要另一个测度参与，而且不一定存在。

当一个变量的取值只有两种可能，如0或1这种时。其中取1的概率被记做$\mu$。因此：

$$ p(x = 1 | \mu) = \mu $$

因此x的概率分布就是伯努利分布，公式表达为：

$$ Bern(x | \mu) = \mu^{x}(1-\mu)^{1-x} $$

其均值和方差为：

$$ E[x] = \mu $$

$$ var[x] = \mu(1-\mu) $$

假设观测数据是i.i.d.的，那么对应的似然函数为：

$$ p(D | \mu) = \prod_{n=1}^{N}p(x_{n} | \mu) = \prod_{n=1}^{N}\mu^{x_{n}}(1-\mu)^{1-x_{n}} $$

通过对对数似然求导得到概率的最大似然估计值：

$$ \mu_{ML} = \frac{1}{N}\sum_{n=1}{N}x_{n} $$

### 2.1.0.1 二项分布

二项分布简单来说就是进行多次实验的伯努利分布。学术上来讲它是求解在给定数据集规模N的条件下，x=1的观测出现数量m的概率分布。公式表述为：

$$ Bin(m | N,\mu) = \frac{N!}{(N-m)!m!}\mu^{m}(1-\mu)^{N-m} $$

对于独立的事件，加和的均值等于均值的加和，嘉禾的方差等于方差的加和。

### 2.1.1 Beta分布

现在我们要求解二项分布中$\mu$的最大似然解，即数据集中x=1出现的比例。为了从贝叶斯的角度看待这个问题，我们需要引入一个关于$\mu$的先验分布$p(\mu)$。

这里首先介绍一下**共轭性，它值一个概率分布的后验概率与先验分布具有相同的函数形式，它能使贝叶斯分析得到极大的简化**。

对于二项分布来说，由于似然函数为$\mu^{x}(1-\mu)^{1-x}$的形式，那么对应的先验分布也得具有这个形式才能满足共轭性，因此我们选择先验分布为Beta分布：

$$ Beta(\mu | a,b) = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1} $$

其中a，b为超参数。Beta分布对应的均值和方差为：

$$ E[\mu] = \frac{a}{a + b} $$

$$ var[\mu] = \frac{ab}{(a+b)^{2}(a + b + 1)} $$

因为先验函数与后验函数满足共轭性，因此后验函数的概率分布为(已归一化)：

$$ p(\mu | m,l,a,b) = \frac{\Gamma(m+a+l+b)}{\Gamma(m+a)\Gamma(l+b)}\mu^{m+a-1}(1-\mu)^{l+b-1} $$

如果我们的目标是尽可能好的预测下一次实验的的输出，那么我们必须估计给定观测数据集D的情况下，x的预测分布。其形式为：

$$ p(x = 1 | D) = \int_{0}^{1}p(x=1 | \mu)p(\mu | D)d\mu = \int_{0}^{1}\mu p(\mu | D)d\mu = E[\mu | D] $$

将前面求得的对应项带入得：

$$ p(x = 1 | D) = \frac{m+a}{m+a+l+b} $$

可以看到，当m,l趋于无穷大时，上式变成了最大似然的结果。因此**贝叶斯的结果和最大似然的结果在数据集的规模区域无穷的情况下会统一到一起**。

并且由Beta分布的方差可知，**在平均来看，随着我们观测到越来越多的数据，后验概率的不确定性将会持续的下降**。公式表示为：

$$ E_{\theta}(\theta) = E_{D}[E_{\theta}[\theta | D]] $$

$$ var_{\theta}[\theta] = E_{D}[var_{\theta}[\theta | D]] + var_{D}[E_{\theta}[\theta | D]] $$

可以看到，**$\theta$的后验方差小于先验的方差。后验均值的方差越大，这个方差的减小就越大**。需要注意的是上式为平均结果，对于一个特定的数据集，有可能后验方差大于先验方差。

## 2.2 多项式变量

二元变量可以用来描述只能取两种可能值中的某一种这样的量。但对于可以取K个互斥状态中的某一个离散变量时，我们需要采用"1-of-K"表示法(类似于one-hot)，则：

$$ x = (0, 0, \dots, 1, \dots, 0)^{T} $$

若用参数$\mu_{k}$表示$x_{k}=1$的概率。那么x的分布就是：

$$ p(x|\mu) = \prod_{k=1}^{K}\mu_{k}^{x_{k}} $$

上式可以看做伯努利分布对于多个输出的一个推广。该分布的均值为：

$$ E[x | \mu] = \sum_{x}p(x | \mu)x = (\mu_{1}, \dots, \mu_{K})^{T} $$

若数有N个独立的观测值数据集，那么对应的似然函数就是各个数据点概率的连乘。

$$ p(D | \mu) = \prod_{k=1}^{K}\mu_{k}^{m_{k}} $$

$\mu$的最大似然解可以通过拉格朗日乘数法得到，其限制条件为$\mu_{k}$的和必须为1：

$$ \sum_{k=1}^{K}m_{k}ln\mu_{k} + \lambda(\sum_{k=1}^{K}\mu_{k}-1) $$

令上式导数为0并将限制条件带入可得最大似然解：

$$ \mu_{k}^{ML} = \frac{m_{k}}{N} $$

它是N次观测中，$x_{k}=1$所占的比例。

当我们考虑
$ m_{1}, m_{2}, \dots, m_{K}$在参数$\mu$和观测总数N条件下的联合分布。根据上面的似然函数公式，我们有：

$$ Mult(m_{1}, m_{2}, \dots, m_{K} |\mu, N) = \frac{N!}{m_{1}!m_{2}!\dots m_{K}!} $$

其中 $\sum_{k=1}^{K}m_{k} = N $。

多项式分布的参数$\mu_{k}$的先验分布为狄利克雷分布：

$$ DIr(\mu | \alpha) = \frac{\Gamma(\alpha_{0})}{\Gamma(\alpha_{1})\dots\Gamma(\alpha_{K})} \prod_{k=1}^{K}\mu_{k}^{\alpha_{k-1}} $$

其中$\alpha$是分布的参数: 

$$\alpha_{0} = \sum_{k=1}^{K}\alpha_{k} $$

用似然函数乘以先验就得到了参数的后验分布(已归一化)：

$$ p(\mu | D,\alpha) = Dir(\mu | \alpha + m) $$

## 2.3 高斯分布

高斯分布的简单定义和性质这里不在赘述，下面我们考虑高斯分布的几何形式。高斯对于x的依赖是通过下面形式的二次型：

$$ \Delta^{2} = (x-\mu)^{T}\Sigma^{-1}(x-\mu) $$

首先我们将$\Sigma$取为对称矩阵，而不失一般性。这是因为任何非对称项都会从指数中消失。那么协方差矩阵的特征向量方程可以写作：

$$ \sum\mu_{i} = \lambda_{i}\mu_{i} $$

我们可以选取特征向量为正交的，因此协方差矩阵可以表示成特征向量的展开的形式：

$$ \Sigma = \sum_{i=1}^{D}\lambda_{i}\mu_{i}\mu_{i}^{T} $$

把上式带入二次型可得：

$$ \Delta^{2}  = \sum_{i=1}^{D}\frac{y_{i}^{2}}{\lambda_{i}} $$

其中 $y_{i} = \mu_{i}^{T}(x-\mu) $

二次型在上式为常数的曲面上为常数，因此高斯密度也是常数。如果所有的特征$\lambda_{i}$都是正数，那么这些曲面表示椭球面(球心位于$\mu$，椭球的轴的方向沿着$\mu_{i}$，沿着轴向的缩放因子为$\lambda_{i}^{\frac{1}{2}}$)。

对于要定义的高斯分布，有必要要求协方差矩阵的所有特征值$\lambda_{i}$严格大于零，否则分布将不能被正确的归一化。一个特征值严格大于零的矩阵被成为正定矩阵。在后面，我们会看到，我们会遇到一个多个特征值为零的高斯分布，那种情况下分布是奇异的，被限制在一个低维的子空间中。如果所有的特征值都是非负的，那么这个矩阵被称为半正定矩阵。

接下来考虑由$y_{i}$定义的新坐标系下高斯分布的形式。从x坐标到y坐标系，我们由一个Jacobian
矩阵J，它的元素为：

$$ J_{ij} = \frac{\partial x_{i}}{\partial y_{j}} = U_{ij} $$

那么在$y_{i}$坐标系中，高斯分布的形式为：

$$ p(y) = p(x)|J| = \prod_{j=1}^{D}\frac{1}{(2\pi\lambda_{j})^{\frac{1}{2}}}exp\{-\frac{y_{j}^{2}}{2\lambda_{j}}\} $$

高斯分布的矩，描述了参数$\mu$和$\sigma$：

$$ E[x] = \mu $$

$$ E[xx^{T}] = \mu\mu^{T} + \Sigma $$

$$ var[x] = \Sigma $$

虽然高斯分布被广泛用作概率密度模型，但它的参数数量巨大，本质上还是单峰的，这给它带来一定的局限性。不过可以通过一定的技术去避免，
