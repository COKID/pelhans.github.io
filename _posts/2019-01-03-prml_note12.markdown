---
layout:     post
title:      "PRML学习笔记（十二）"
subtitle:   "第十二章 连续潜在变量"
date:       2019-01-03 00:15:18
author:     "Pelhans"
header-img: "img/prml.jpg"
header-mask: 0.3 
catalog:    true
tags:
    - PRML
---


> PRML 和 ESL 的学习基本上是学十得一。稳扎稳打再来一次

* TOC
{:toc}

# 连续潜在变量

## 12.1 主成分分析

主成分分析，或者称为PCA，是一种被广泛使用的技术，应用领域包括维度降低。有损数据压缩、特征抽取、数据可视化。它也被成为 Karhunen-Loeve 变换。有两种经常使用的PCA的定义，他们会给出同样的算法。**PCA可以被定义为数据在低维线性空间上的正交投影，这个线性空间被称为主子空间，使得投影数据的方差被最大化。等价地，它也可以被定义为使得平均投影代价最小的线性投影。平均投影代价是指数据点和它们投影之间的平均平方距离**。

### 12.1.1 最大方差形式

假设有一组观测数据集$x_{n}$，维度为D，目标是将数据投影到维度为M(M<D)的空间中，同时最大化投影数据的方差。

考虑在一维空间上的投影，我们可以使用D维向量$u_{1}$定义这个空间的方向。为了方便，我们假定选择一个单位向量，这样，每个数据点$x_{n}$被投影到一个标量值$u_{1}^{T}x_{n}$上。投影数据的均值是$u_{1}^{T}\bar{x}$，投影数据的方差为：

$$ \frac{1}{N}\sum_{n=1}^{N}\{u_{1}^{T}x_{n} - u_{1}^{T}\bar{x}\}^{2} = u_{1}^{T}Su_{1} $$

我们现在关于$u_{1}$最大化投影方差$u_{1}^{T}Su_{1}$。采用拉格朗日乘数法，以u的归一化条件为限制，我们看到驻点满足：

$$ Su_{1} = \lambda_{1}u_{1} $$

这表明$u_{1}$一定是S的一个特征向量。如果我们左乘$u_{1}^{T}$，使用$u_{1}^{T}u_{1}=1$，我们看到方差为：

$$u_{1}^{T}Su_{1} = \lambda_{1} $$

因此当我们**将$u_{1}$设置为与最大的特征值$\lambda_{1}$的特征向量相等时，方差会达到最大值，这个特征向量被称为第一主成分**。对于其他主成分，我们可以考虑那些与现有方向正交的所有可能方向中，将新的方向选择为最大化投影方差的方向。以此类推得到协方差矩阵S的M个特征向量$u_{1},\dots,u_{M}$，对应于M个最大特征值$\lambda_{1},\dots,\lambda_{M}$。

### 12.1.2 最小误差形式

现在考虑基于误差最小化的投影方法。为此我们引入D维基向量的一个完整的单位正交集合$u_{i}$，其中$i=1,\dots,D$，且满足：

$$ u_{i}^{T}u_{j} = \delta_{ij} $$

由于基是完整的，因此数据点可以表示为基向量的线性组合，因此：

$$ x_{n} = \sum_{i=1}^{D}(x_{n}^{T}u_{i})u_{i} $$

然而，我们的目标是使用限定数量M个变量的一种表示方法来近似数据点，这对应于在低维子空间上的投影，若采用M个基向量来表示M为子空间，那么我们可以用下式来近似每个数据点：

$$ \tilde{x}_{n} = \sum_{i=1}^{M}z_{ni}u_{i} + \sum_{i=M+1}^{D}b_{i}u_{i} $$

其中${z_{ni}}$依赖于特定的数据点，而 $b_{i}$是常数，对于所有的数据点都相同。为了最小化失真，我们采用原始数据点和与它近似点$\tilde{x}_{n}$之间的平方距离，在数据集上取平均，即最小化下式：

$$ J = \frac{1}{N}\sum_{n=1}^{N}||x_{n} - \tilde{x}_{n} ||^{2} $$

消去上式中的$z_{ni}$和$b_{i}$，则得到纯粹关于$u_{i}$的J的表达式：

$$ J = \sum_{i=M+1}^{D}u_{i}^{T}Su_{i} $$

对于任意的D和M<D，最小化J的解一般都可以通过将$u_{i}$选择为协方差矩阵的特征向量的方式的得到，即：

$$ Su_{i} = \lambda_{i}u_{i} $$

这样，J就变成：

$$ J = \sum_{i=M+1}^{D}\lambda_{i} $$

这就是与主子空间正交的特征值的加和，于是，我们可以通过将这些特征向量选择成D-M个最小的特征值对应的特征向量，来得到J的最小值，因此**定义了主子空间的特征向量是对应于M个最大特征值的特征向量**。
