---
layout:     post
title:      "PRML学习笔记（十）"
subtitle:   "第十章 近似推断"
date:       2018-12-05 00:15:18
author:     "Pelhans"
header-img: "img/prml.jpg"
header-mask: 0.3 
catalog:    true
tags:
    - PRML
---


> PRML 和 ESL 的学习基本上是学十得一。稳扎稳打再来一次

* TOC
{:toc}

# 近似推断

对于实际应用中的许多模型来说，计算后验概率分布或者计算关于这个后验概率分布的期望是不可行的。这可能由于潜在空间的维度太高无法直接计算，或者后验概率分布的形式特别复杂，无法直接计算。此时需要借助近似方法。近似方法分为两大类，一类是随机方法，如11章的马尔科夫链蒙特卡洛方法，这类方法在给定无限多的计算资源时，他们可以生成相当精确的结果，近似的来源是使用了有限的处理时间。另一类是确定性近似方法，这些方法基于对后验概率分布的解析近似，例如通过假设后验概率分布可以通过一种特定的方式分解，或者假设后验概率分布有一个具体的参数形式，该方法永远无法生成精确地解。

## 10.1 变分推断

我们可以简单地将泛函作为一个映射，它以有一个函数作为输入，返回泛函的值作为输出。传统微积分中一个常见的问题是找到一个x使得y(x)取得最大值或最小值。类似地，变分法中，我们寻找一个函数y(x)来最大化或最小化泛函F[y]。即，对于所有可能的函数y(x)，我们想找到一个特定的函数，使得F[y]达到最大值或者最小值。

虽然变分法本质上并没有任何近似的东西，但它们通常会被用于寻找近似解。寻找近似解可以通过限制需要最优化算法搜索的函数的范围来完成。在概率推断的应用中，限制条件的形式可以是可分解的假设。

现在我们讨论变分最优化的概念如何应用于推断问题。假设我们有一个纯粹的贝叶斯模型，其中每个参数都有一个先验概率分布。所有潜在变量和参数的集合记做Z，观测变量的集合记做X。概率模型确定了联合分布p(X, Z)，**我们的目标是找到对后验概率分布
$p(Z | X) $ 以及模型证据p(X)的近似。** 

类似于EM，将对数边缘概率分解，有：

$$ \ln p(X) = \mathcal{L}(q) + KL(q || p) $$

$$ \mathcal{L}(q) = \int q(Z) \ln\left\{ \frac{p(X, Z)}{q(Z)} \right\} dZ $$

$$ KL(q || p) = -\int q(Z)\ln \left\{p(Z | X)q(Z)  \right\} dZ $$

与之前一样，我们可以通过关于概率分布q(Z)的最优化来使得$\mathcal{L}}(q)达到最大值，这等价于最小化KL散度。现在假设在需要处理的模型中，对真实的概率分布进行操作是不可行的。于是，我们转而考虑概率分布q(Z)的一个受限制的类别，然后寻找这个类别中使得KL散度达到最小值的概率分布。我们的目标是充分限制q(Z)可以取得的概率分布的类别范围，使得这个范围中的所有概率分布都是可以处理的概率分布。同事还要使得这个范围充分大、充分灵活，从而它能够提供对真是后验概率分布的一个足够好的近似。

## 10.1.1 分解概率分布

现在，我们限制概率分布q(Z)的范围，**假设我们将Z的元素划分为若干个互不相交的组，记做$Z_{i}$**，其中每个参数都有一个先验概率分布$i=1,\dots,M$、然后，我们假定q分布关于这些分组可以进行分解，即：

$$ q(Z) = \prod_{i=1}^{M}q_{i}(Z_{i}) $$

变分推断的这个分解的形式对应于物理学中的一个近似框架，叫平均场理论。

在所有具有上式的形式的概率分布q(Z)中，我们现在寻找下界$\mathcal{L}(q)$最大的概率分布。于是，我们希望对$\mathcal{L}(q)$关于所有的概率分布$q_{i}(Z_{i})$进行一个自由行是的(变分)最优化，我们将**通过每个因子进行最优化来完成整体的最优化过程**。为此我们分理出依赖于一个因子$q_{j}(Z_{j})$的项：

$$ \mathcal{L}(q) = \int q_{j}\ln \tilde{p}(X, Z_{j}) - \int q_{j}\ln q_{j}dZ_{j} + const $$

$$ \ln \tilde{p}(X, Z_{j}) = E_{i\neq j}[\ln p(X, Z)] + const $$

其中$E_{i\neq j}[\dots]$表示关于定义在所有$z_{i}(i\neq j)$上的q概率分布的期望，即：

$$ E_{i\neq j}[\ln p(X, Z)] = \int \ln p(X,Z)\prod_{i\neq j}q_{i}dZ_{i} $$

假设我们保持$\{q_{i\neq j}  \}$固定，关于概率$q_{j}(Z_{j})$的所有可能的形最大化$\mathcal{L}(q)$。这很容易做，因为最大化$\mathcal{L}(q)$等加入最小化KL散度，且最小值出现在$q_{j}(Z_{j}) = \tilde{p}(X, Z_{j})$的位置，于是，我们得到了最优解$q^{\*}(Z_{j})$的一般表达式：

$$ \ln q^{\*}_{j}(Z_{j}) = E_{i\neq j}[\ln p(X, Z)] + const $$

上式很重要，它是变分方法应用的基础。这个解表明，为了得到因子$q_{j}$的最优解的对数，我们只需要考虑所有隐含变量和课件变量上的联合概率分布的对数，然后关于所有其他的因子$\{q_{i} \}$取期望即可，其中$i\neq j$。在实际应用中，我们发现直接对上式进行操作，而后在必要的时候通过观察的方式恢复出归一化系数是较为方便的做法。


## 10.2 例子：高斯的变分混合

和前面的记号一致，$\pi$为混合系数，对于每个观测$x_{n}$有对应的潜在变量$z_{n}$，它是一个"1-of-K"的二值向量，观测数据集记做X，因此，Z的条件概率分布为：

$$ p(Z | \pi) = \prod_{n=1}^{N} \prod_{k=1}^{K} \pi_{k}^{z_{nk}} $$ 

类似地，观测数据向量的条件概率分布为：

$$ p(X | Z, \mu, \Lambda) = \prod_{n=1}^{N} \prod_{k=1}^{K}\mathcal{N}(x_{n} | \mu_{k}, \Lambda_{k}^{-1})^{z_{nk}} $$

需要注意的是我们使用的是精度矩阵而非协方差矩阵。接下来，我们引入参数$\mu,\Lambda, \pi$上的先验概率分布，这里我们采用共轭先验。因此对于参数$\pi$：

$$ p(\pi) Dir(\pi | \alpha_{0}) C(\alpha_{0})\prod_{k=1}^{K}\pi_{k}^{\alpha_{0} - 1} $$

为了对称性，为每一个分量选择相同的参数。

对于均值和精度：

$$ \p(\mu, \Lambda) = p(\mu | \Lambda)p(\Lambda) = \prod_{k=1}^{K}\mathcal{N}(\mu_{k} | m_{0}, (\beta_{0}\Lambda)^{-1})\mathcal{W}(\Lambda_{k} | W_{0}, \nu_{0}) $$

### 10.2.1 变分分布

高斯模型的贝叶斯混合的有向图如下图所示：

![](/img/in-post/prml_note8/p10.png)

因此，所有随机变量的联合分布为：

$$ p(X, Z, \pi, \mu, \Lambda) = p(X | Z, \mu, \Lambda)p(Z | \pi)p(\pi)p(\mu | \Lambda)p(\Lambda) $$

现在考虑一个变分分布，它可以在潜在变量与参数之间进行分解，即：

$$ q(Z, \pi, \mu, \Lambda) = q(Z)q(\pi, \mu, \Lambda) $$

经过上述假设得到的分解，因子q(Z)和$q(\pi, \mu, \Lambda)$的函数形式会在变分分布的最优化过程中自动确定。

首先让我们求解因子$q(Z)$的更新方程，根据前面的公式，最有因子的对数为：

$$ \ln q^{\*}(Z) = E_{\pi, \mu,\Lambda}[\ln p(X, Z, \pi, \mu, \Lambda)] + const $$

保留右侧与Z有关的项，则：

$$ \ln q^{\*}(Z) = E_[\pi][\ln p(Z | \pi)] + E_{\mu, \Lambda}[\ln p(X | Z,\mu, \Lambda)] + const $$

带入相关分布，去掉与Z无关的项则有：

$$ q^{\*}(Z) \propto \prod_{n=1}^{N}\propto_{k=1}^{K}\rho_{nk}^{z_{nk}} $$

其中：

$$ \ln \rho_{nk} = E[\ln \pi_{k}] + \frac{1}{2}E[\ln|\Lambda_{k}|] - \frac{D}{2}\ln(2\pi) - \frac{1}{2}E_{\mu_{k}, \Lambda_{k}}[(x_{n} - \mu_{k})^{T}\Lambda_{k}(x_{n} - \mu_{k})] $$

我们要求这个概率分布是归一化的，并且对于每个n值，$z_{nk}$是二值的，且加和为1.则：

$$ q^{\*}(Z) = \prod_{n=1}^{N}\propto_{k=1}^{K} r_{nk}^{z_{nk}} $$

$$ r_{nk} = \frac{\rho_{nk}}{\sum_{j=1}^{K}\rho_{nj}} $$

对于离散概率分布$q^{\*}(Z)$，我们有标准的结果：

$$ E[z_{nk}] = r_{nk} $$

可以看到，$r_{nk}$扮演着“责任”的角色。同时，$q^{\*}$的最优解依赖于关于其他变量计算得到的矩，因此与之前一样，变分更新方程是耦合的，必须用迭代的方式求解。

同理，对于变分后验概率分布中的因子q(\pi, \mu, \Lambda)$：

$$ \ln q^{\*}(\pi, \mu, \Lambda) = \ln p(\pi) \sum_{k=1}^{K}\ln p(\mu_{k}, \Lambda_{k}) + E_{Z}[\ln p(Z | \pi)] + \sum_{k=1}^{K}\sum_{n=1}^{N}E[z_{nk}]\ln \mathcal{N}(x_{n} | \mu_{k}, \Lambda_{k}^{-1}) + const $$

我们发现$\pi$和其他两个没有耦合，因此可以得到对应的解：

$$ q^{\*}(\pi) = Dir(\pi | \alpha) $$

其中 $\alpha$ 的元素为 $\alpha_{k}$，形式为：

$$ \alpha_{k} = \alpha_{0} + \sum_{n=1}^{N}r_{nk} $$

对于变分后验概率分布$q^{\*}(\mu_{k}, \Lambda_{k})$无法分解成边缘概率分布的乘积，但我们可以将其分解为：

$$ q^{\*}(\mu_{k}, \Lambda_{k}) q^{\*}(\mu_{k} | \Lambda_{k})q^{\*}(\Lambda_{k}) $$ 

观察对应的项，可得$\mu_{k}$和 $\Lambda_{k}$。结果是一个高斯-Wishart分布，形式为：

$$ q^{\*}(\mu_{k}, \Lambda_{k}) = \mathcal{N}(\mu_{k} | m_{k}, (\beta_{k}\Lambda_{k})^{-1})\mathcal{W}(\Lambda_{k} | W_{k}, \nu_{k}) $$

其中：

$$ \beta_{k} = \beta_{0} + N_{0} $$

$$ m_{k} = \frac{1}{\beta_{k}}(\beta_{0}m_{0} + N_{k}\hat{x}_{k}) $$

$$ W_{k}^{-1} = W_{0}^{-1} + N_{k}S_{k} + \frac{\beta_{0}N_{k}}{\beta_{0} + N_{k}}(\hat{x}_{k} - m_{0})(\hat{x}_{k} - m_{0})^{T} $$

$$ \nu_{k} = \nu_{0} + N_{k} $$

可以看到，更新方程类似于EM的M步骤。

因此变分后验概率分布的最优化涉及到在两个阶段之间进行循环，这两个阶段类似于最大化似然的E步骤和M步骤。在变分推断与E步骤等价的步骤中，我们使用当前状态下模型参数上的概率分布来计算关于变分分布的参数的期望中的各阶矩，从而计算$E[z_{nk}]=r_{nk}$然后，在接下来的M步骤等价的步骤中，我们令这些“责任”保持不变，然后使用它们通过变分推断得到的$q^{\*}(\pi)$和$q^{\*}(\mu_{k}, \Lambda_{k})$ 重新计算参数上的变分分布。

### 10.2.2 变分下界

对于高斯分布的变分混合，下界为：

$$ \mathcal{L} = \sum_{Z}q(Z, \pi, \mu, \Lambda)\ln\left\{\frac{p(X, Z, \pi, \mu, \Lambda)}{q(Z, \pi, \mu, \Lambda)}\right\} d\pi d\mu d\Lambda $$

将其进行变分分解并对每项进行分别计算即可。值得注意的是，下界提供了另一种推导变分重估计方程的方法，即将下界作为概率分布的参数的函数，关于这些参数的最大化下界就会得到所需的重估计方程。

### 10.2.3 预测概率密度

我们通常对观测变量的新值$\hat{x}$的预测概率密度感兴趣，与这个观测相关联的有一个潜在变量$\hat{z}$，从而预测概率分布为：

$$ p(\hat{x} | X) = sum_{\hat{Z}}\int\int\int p(\hat{X} | \hat{z},\mu, \Lambda)p(\hat{z} | \pi)p(\pi, \mu, \Lambda | X)d\pi d\mu d\Lambda $$

对真是后验概率分布
$p(\pi, \mu, \Lambda | X)$用它的变分近似$q(\pi)q(\mu, \Lambda) $ 替换的方式来近似预测概率分布，从而得到近似解。

## 10.5 局部变分方法


