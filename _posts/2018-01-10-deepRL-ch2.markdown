---
layout:     post
title:      "David Silver 强化学习 第二讲" 
subtitle:   "马尔可夫决策过程"
date:       2018-01-10 09:35:18
author:     "Pelhans"
header-img: "img/post_deepRL.jpg"
header-mask: 0.3 
catalog:    true
tags:
    - Reinforment Learning
---


> MDP是对完全可观测环境描述的，也就是说观测到的状态内容完整的决定了决策需要的特征。几乎所有学习问题都可以转化为MDP。

* TOC
{:toc}

#  简介

**什么是马尔可夫过程？**

马尔可夫过程是一个无记忆的随机过程，可以用一个元组<S, P>表示，其中S是有限数量的状态集，P是状态转移概率矩阵。这里吐槽一下。之前接触的转移概率都用T表示，这里用P还真不适应。。。。

## 马尔科夫奖励过程

马尔科夫奖励过程在马尔科夫过程的基础上增加了奖励R和衰减系数$$\gamma$$，即<S, P, R, $$\gamma$$>。

### 奖励 

S状态下的奖励是某一时刻t在状态s下，在t+1时刻能获得的奖励期望：

$$ R_{s}= E[R_{t+1} | S_{t} = s] $$

在表述上可理解为“奖励是当进入某个状态会获得相应的奖励”。

### 衰减系数

衰减系数(DIscount Factor) $$\gamma \in [0,1]$$（每次输入$$\gamma$$好麻烦啊。。。后面用r代替吧。。。)衰减系数是对远期利益的衰减。对于它的引入有各种各样的解释，如避免陷入无限循环、远期利益具有一定的不确定性等等。下图给出学生MRP的示例：

![](/img/in-post/deepRL_ch2/deepRL_ch2_1.jpg)

由上图可以看出，相比于单马尔科夫过程，在每个状态上增加了红色的奖励R，表示进入该状态能够获得的奖励。

### 收获 Return

**收获$$G_{t}$$的定义为在一个马尔科夫链上从t时刻开始往后所有的奖励的有衰减的总和**。公式表示为:

$$G_{t} = R_{t+1} + \gamma R_{t+2} + \ldots = \Sigma_{k=0}^{\infty}R_{t+k+1}$$

其中衰减系数体现了未来的奖励在当前时刻的价值比例，r越接近0则表明更注重短期利益，反之则租用长期利益。

### 价值函数

**价值函数给出了某一状态或某一行为的长期价值，其定义为从该状态开始的马尔科夫链收获的期望**，公式表示为：

$$v(s) = E[G_{t} | S_{t} = s] $$

需要注意的是价值可以描述状态，也可以描述某状态下的某个行为，在本公开课中，**采用状态价值函数或价值函数来描述针对状态的价值**；用行为价值函数来描述某一状态下执行某一行为的价值，严格意义上说行为价值函数是“状态行为”对价值函数的简写。

#### 来个例子

看完上面的定义一定很懵吧。。。傻傻分不清吧～现在通过一个学生MRP来说明。

下图将图1的例子表格化，其中Reward表示进入该状态能够获得的奖励，如C1下面的-2表示进入状态C1就能获得奖励-2.奖励下面的数表示状态转移概率，其中竖向的是发出概率，横向的为接收概率，因此横向概率和为1.

![](/img/in-post/deepRL_ch2/deepRL_ch2_2.jpg)

考虑下图中的几个马尔科夫链，当$$\gamma = \frac{1}{2}$$时，在t=1时刻($$S_{1} = C_{1}$$)的状态$$S_{1}$$的收获

![](/img/in-post/deepRL_ch2/deepRL_ch2_3.jpg)

从上表也可以看到，收获是针对马尔科夫链中的某一个状态来说的。而价值则可以看做某一状态收获的期望。下图给出当r=0.9时的状态价值。

![](/img/in-post/deepRL_ch2/deepRL_ch2_4.jpg)

### 价值函数的推导

#### Bellman方程-MRP

由价值的的定义公式可知:

![](/img/in-post/deepRL_ch2/deepRL_ch2_5.jpg)

由此得到针对MRP的Bellman方程：

$$ v(s) = E[R_{t+1} + \gamma v(S_{t+1}) | S_{t} = s] $$

由方程可以看出，_v(s)_有两部分组成，一个是该状态的即时奖励期望，另一个是下一时刻状态的价值期望，可以根据下一时刻状态的概率分布得到其期望。

