---
layout:     post
title:      "David Silver 强化学习 第四讲" 
subtitle:   "Model-Free 的预测"
date:       2017-01-16 21:35:18
author:     "Pelhans"
header-img: "img/post_deepRL.jpg"
header-mask: 0.3 
catalog:    true
tags:
    - Reinforment Learning
---


> 在之前的学习中，我们学习的都是在清楚MDP细节时的策略评估和优化，在本讲以及下讲，我们讲学习解决一个可以被认为时MDP但是不掌握MDP具体细节的问题。此时Agent直接通过与环境的交互来进行控制和预测。本讲集中与预测部分，即策略评估。并介绍三种处理方法：蒙特卡罗强化学习、时序差分强化学习和$$\lambda$$时序差分强化学习。

* TOC
{:toc}

#  蒙特卡罗强化学习(Monte-Carlo Reinforcement Learning)

蒙特卡罗强化学习指的是在不清楚MDP状态及即时奖励的情况下，直接中完整的Episode来学习状态价值，通常情况下某个状态的价值等于多个Epiode中该状态算得到的所有收获的平均。它的特点有：不基于模型本身、直接从经历过的Episode中学习、**使用的思想是用平均收获代替价值**，理论上来说，经历的Episode越多，结果就越准确。

## 蒙特卡罗策略评估(Mone-Carlo Policy Evaluation)

该策略评估的目标是在给定的策略下，从一系列完整的Episode经历中学习得到该策略下的状态价值函数。它需要使用的信息包含状态的转移、使用的行为序列、中间状态获得的即时奖励以及到达终止状态时获得的即时奖励。它们可以从完整的Episode中获取。数学描述为:

基于给定策略$$\pi$$的一个Episode信息可以表示为如下的一个序列：

$$S_{1},A_{1},R_{2},S_{2},A_{2},\ldots ,S_{t},A_{t},R_{t+1},\ldots,S_{k} /~ \pi$$

t时刻的状态$$S_{t}$$的收获:

$$ G_{t} = R_{t+1} + \gamma R_{t+2} + \gamma^{T-1}R_{T}$$

其中T为终止时刻。该策略下某一状态s的价值：

$$ v_{\pi}(s) = E_{\pi}[G_{t} | S_{t} = s] $$

即该状态下收获的期望。

在状态转移过程中，会出现一个状态又一次或多次返回自身的情况，此时的状态次数计算和Episode的收获计算为：

1） 首次访问蒙特卡罗策略评估

即对于每一个Episode，仅当该状态第一次出现时列入计算：

状态出现的次数加1： $$ N(s) \leftarrow N(s) + 1 $$

总的收获值更新： $$S(s) \leftarrow S(s) + G_{t} $$

状态s的价值： $$ V(s) = S(s) / N(s) $$

当$$ N(s) \rightarrow \infty$$时，$$ V(s) \rightarrow v_{\pi}(s) $$

2） 每次访问蒙特卡罗策略的评估

状态s每次出现在状态转移链时，计算的具体公式和上面的一样，但具体意义不一样**原文是这么写的，但我也不知道为啥会一样。。。按道理上面的第一步不应该计算状态出现的次数啊**。

状态出现的次数加1： $$ N(s) \leftarrow N(s) + 1 $$
 
总的收获值更新： $$S(s) \leftarrow S(s) + G_{t} $$
 
状态s的价值： $$ V(s) = S(s) / N(s) $$
 
当$$ N(s) \rightarrow \infty$$时，$$ V(s) \rightarrow v_{\pi}(s) $$

## 累进更新平均值(Incremental Mean)

前面我们提到需要用蒙特卡罗方法求解平均收获时，需要计算平均值，通常需要现存储所有数据然后计算，但这样存储起来就很麻烦，因此下面给出一种实时更新平均值的方法。理论公式如下:

$$\mu_{k} = \frac{1}{k}\sum\limits_{j=1}^{k}x_{j}$$

$$ = \frac{1}{k}(x_{k} + \sum\limits_{j=1}^{k-1}x_{j})$$

$$ = \frac{1}{k}(x_{k} + (k-1)\mu_{k-1})$$

$$ = \mu_{k-1} + \frac{1}{k}(x_{k} - \mu_{k-1})$$

公式推到比较简单，把该方法用于蒙特卡罗策略评估就可以得到实时更新方案了。

## 蒙特卡罗累进更新

对于每个Episode中的每一个：$$S_{1}, A_{1},R_{2},S_{2},A_{2},\ldots,S_{t},A_{t},R_{t+1},\ldots,S_{k}$$

对于Episode中的每一个状态$$S_{t}$$有一个收获$$G_{t}$$，每一次碰到$$S_{t}$$，使用上述方法对状态的平均价值$$V(S_{t})$$进行更新：

$$V(S_{t}) \leftarrow V(S_{t}) + \frac{1}{N(S_{t})}(G_{t} - V(S_{t}))$$

其中:$$N(S_{t}) \leftarrow N(S_{t}) + 1$$

在处理非静态问题时，使用累进更新可以扔掉过往Episode的信息。此时引入参数$$\alpha$$来更新状态价值：

$$ V(S_{t}) \leftarrow V(S_{t}) + \alpha(G_{t} - V(S_{t})) $$

以上就是蒙特卡罗学习方法，但由于该方法必须经历完整的Episode，因此在实际中使用的不多，接下来介绍实际中常用的时序差分学习方法(TD方法).

## 时序差分学习 （Temporal-Difference Learning)

