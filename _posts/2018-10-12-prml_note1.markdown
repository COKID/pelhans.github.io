---
layout:     post
title:      "PRML学习笔记（一）"
subtitle:   "第一章 绪论"
date:       2018-10-12 00:15:18
author:     "Pelhans"
header-img: "img/prml.jpg"
header-mask: 0.3 
catalog:    true
tags:
    - PRML
---


> PRML 和 ESL 的学习基本上是学十得一。稳扎稳打再来一次

* TOC
{:toc}

# 绪论

## 1.1 例子：多项式曲线拟合

这是一个简单的回归问题，假设数据集由函数 $sin(2\pi x)$ 产生。目标变量带有随机的噪声。现假定有一个训练集，这个训练集由x的N次观测组成，写作 $ x = (x_{1}, \dots, x_{N})^T $，伴随着对应的t 的观测值，记做 $ t = (t_{1}, \dots, t_{N})^T $。我们的目标是利用这个训练集预测对于输入变量的新值 $\hat{x}$的目标变量的值 $\hat{t}$。

最简单的情况是使用多项式函数来拟合数据：

$ y(x,w) = w_{0} + w_{1} x + w_{2} x^{2} + \dots + w_{M}x^{M} = \sum_{j=0}^{M}w_{j}x^{j} $

其中M是多项式的阶数，$x^{j}$表示x的j次幂。多项式系数$w_{0}$等记做向量w。这里我们通过最小化误差函数的方法实现系数值的确定。误差函数衡量了对于任意给定的w值，函数y(x,w)与训练数据集数据的差别。一个比较简单且用途广泛的误差函数为：

$ E(w) = \frac{1}{2}\sum_{n=1}^{N}{y(x_{n},w) - t_{n}}^{2} $

选择多项式的阶数M也是一个问题。M过小则模型的拟合效果太差，而M过大又使得模型过于复杂导致过拟合的出现。而且，通过实验发现，随着M的增大，系数的大小通常会变大。从直觉上讲，我们看到**有着更大的M值的更灵活的多项式被过分地调参，使得多项式被调节成了与目标值的随机噪声相符。**

实际上，寻找模型参数的最小平方方法代表了最大似然的一种特殊情形，并且过拟合问题可以被理解为最大似然的一个通用属性。通过使用一种贝叶斯方法，过拟合问题可以被避免。

另一方面，除贝叶斯方法外，经常用来控制过拟合现象的一种技术是正则化：即给误差函数增加一个惩罚项，使得系数不会达到很大的值。惩罚项最简单的情况就是采用所有系数的平方和形式。修改后的误差函数为：

$$ \hat{E}(w) = \frac{1}{2}\sum_{n=1}^{N}{y(x_{n},w) - t_{n}}^{2} + \frac{\lambda}{2} ||w||^{2} $$

其中 

$$ ||w||^{2} = w^{T}w = w_{0}^2 + w_{1}^{2} + \dots + w_{M}^{2} $$

对于模型复杂度，除使用贝叶斯方法外，也可以通过在测试集中分离出一部分作为验证集来最优化复杂度。

## 1.2 概率论

> 太基础的概率就不写了。。。

概率论有两个比较重要的规则：加和规则和乘积规则。其中加和规则为：

$ p(X = x_{j}) = \sum_{j=1}^{L}p(X = x_{i}, Y = y_{j}) $

其中 $ p(X = x_{j}) $有时被成为边缘概率。乘积规则对应为：

$$ p(X = x_{i}, Y = y_{j}) =  p(Y = y_{j} | X = x_{i})p(X = x_{i}) $$

由此可以推出**贝叶斯定理**：

$$ P(Y | X) = \frac{p(X | Y)p(Y)}{p(X)} = \frac{p(X | Y)p(Y)}{\sum_{Y}p(X|Y)p(Y)} $$

### 1.2.1 概率密度

如果一个实值变量x的概率落在区间$ (x, x+\delta x) $ 的概率由 $p(x)\delta x$给出，那么 p(x)叫做x的概率密度。对概率密度进行积分可以得到x位于区间内的概率：

$ p(x \in (a,b)) = \int_{a}^{b} p(x)dx $

在变量以非线性的形式变换的情况下，概率密度函数通过 Jacobian因子变换为与简单函数不同的形式。这个性质的一个结果是，概率密度最大的值取决于变量的选择。

如果x是一个离散变量，那么p(x)有时被叫做概率质量函数。

### 1.2.2 期望和方差
