---
layout:     post
title:      "语音识别笔记 (四)" 
subtitle:   "基于GMM-HMM的自动语音识别框架"
date:       2017-01-12 10:29:28
author:     "Pelhans"
header-img: "img/post_deepspeech_ch1_ch2.jpg"
header-mask: 0.3 
catalog:    true
tags:
    - ASR
    - NLP
---


> 尽管基于GMM-HMM的语音识别模型已基本被神经网络所取代，但其背后的思想和处理方式仍需要我们仔细学习。

* TOC
{:toc}

# 第四讲

自动语音识别(automaic speech recognition)就是建立一个将声学信号转化为文字的系统,而自动语言理解则更进一步,它需要对句子的含义进行理解.

一个性能良好的ASR系统面临如下挑战:

1) 语音识别任务中存在大量的词汇.

2) 语音的连续性,流畅性,通用性.

3) 信号道以及噪音问题.

4) 说话者的口音.

因此在本讲及下讲我们讲介绍一个简单的大尺量ASR系统(LVCSR).

## 语音识别系统的结构

### 噪声道模型

基于HMM的语音识别系统讲语音识别任务看做"噪声道模型".即若我们知道噪声是如何扰乱源信号的话,我们就能通过尝试每个可能句子来找到正确的源句子对应的波形,而后判断其与输出的匹配程度.下图为噪声道模型的图示.

![](/img/in-post/deepspeech_ch4/deepspeech_ch4_1.jpg)

假如最终我们采用概率来评估结果的好坏,那么就讲语音识别问题转化为贝叶斯推断的特例.同事由于英语中的状态空间会很大,因此我们不能完全对其遍历.这也就带来了解码的问题.幸好随着技术的发展,我们可以使用Viterbi算法来方便的进行解码.

### 语音识别中的基本公式

为了表明问题,我们先定义一些变量.

1) 声学输入O定义为HMM中的可观测量.通常来讲,它是通过对波形文件进行分帧处理得到的声学特征向量.

$$O = o_{1}, o_{2}, o_{3},\ldots , o_{t}$$

2)句子是由一系列的单词w组成的:

$$W = w_{1}, w_{2}, w_{3},\ldots , w_{n}$$

有了以上的定义,根据ASR的任务要求,我们要在给定声学输入O的情况下找出最有可能的字符串输出W,用公式表达为:

$$W^{max} = \arg\max\limits_{W\in L}P(W | O) $$

因此我们要计算
$$P(W | O) $$,利用贝叶斯公式得:

$$W^{max} = \arg\max\limits_{W\in L} \frac{P(W | W) P(W)}{P(O)}$$

由于我们要针对W进行优化,而输入O是不变的,因此公式简化为:

$$W^{max} = \arg\max\limits_{W\in L}P(O | W)P(W)$$

在上式中,
$$P(O | W)$$成为观察序列的似然值(Likelihood),由声学模型(Acoustic Model, AM)提供,P(W)称为先验概率,由语言模型(Language model, LM)提供.

对于LM的获得由多种途径,LM的形式也多种多样,一般情况下我们采用N-gram作为我们的语言模型,N取值在3-5之间.之前我使用的百度Deepspeech框架和谷歌Wavenet框架的语言模型都是通过Kenlm获取的N-gram语言模型.

### 语音识别框架

下图给出语音识别系统的三个步骤:

![](/img/in-post/deepspeech_ch4/deepspeech_ch4_2.jpg)

第1阶段为特征抽取阶段或称之为信号处理阶段.在该阶段我们先将声学波形文件按时间切分为一帧一帧的片段,一般以10/15/20 毫秒为一帧时间.在分为帧后我们通过变化将其转化为特征频谱,对于每帧我们用39个特征向量来表示它.(在采用MFCC时是13*3=39个特征,后面会讲到).

第二阶段是声学模型建立或音素识别阶段.在该阶段我们对特征向量计算似然值,如采用高斯混合模型(Gaussian Mixture Model, GMM)来计算HMM的隐态q(对应于音素或子音素)的似然值
$$P(o | q)$$.

第三阶段是解码阶段,我们将声学模型和HMM的单词发音词典和语言模型结合来得到最优可能 的文本序列.之前大部分的ASR系统都是用Viterbi算法进行解码(真的很好用...).

## HMM在语音识别中的应用

在语音识别任务中,隐藏状态q对应于音素,部分音素或单词.可观察量是波形文件帧的频谱和能量信息.解码过程就是将声学信息映射到音素和单词.

下图给出基本的HMM音素状态结构概览,例子中的单词是"six":

![](/img/in-post/deepspeech_ch4/deepspeech_ch4_3.jpg)

这个单词由四个音素组成(emitting states),加上开始和终止状态.转移概率A表示状态间的转移,观察概率B表示每个音素的似然度.需要注意的是除初始和终止状态外,音素状态可以向下一个音素状态转移,也可以转移到自身.但不能向前一个状态转移,这种从左到右的HMM结构叫做Bakis网络.状态循环回自身允许模拟单因素持续时间较长的情况.
