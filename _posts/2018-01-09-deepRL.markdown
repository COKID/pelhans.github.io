---
layout:     post
title:      "David Silver 强化学习 第一讲" 
subtitle:   "强化学习简介"
date:       2018-01-09 21:57:18
author:     "Pelhans"
header-img: "img/post_deepRL.jpg"
header-mask: 0.3 
catalog:    true
tags:
    - Reinforment Learning
---


> 阿尔法狗真真切切的让普通民众感受到了强化学习的威力。

* TOC
{:toc}

#  简介

每天坐地铁无聊，打游戏又没有双手的地，所以没事只能看B乎，无意中发现[叶强的强化学习专栏](https://zhuanlan.zhihu.com/reinforce)。它对David Silver的公开课进行了文字版的总结，写的非常棒。但其中有一些知识点已经知道对自己没用，还有更多里面没有提及但自己不懂的东西，因此打算大体上沿着他的框架把自己需要的部分记录下来，同时把自己查的、思考的也加上方便自己以后复习。感谢原作者。

# 什么是强化学习

强化学习是机器学习的一个分支,其特点为:

	1:没有监督学术,只有奖励信号.
	2:奖励信号不一定是实时的,很有可能是延后的,有时甚至延后很多.
	3:时间(序列)是一个重要因素.
	4:当前的行为影响后续接收到的数据.

强化学习主要基于一个"奖励假设",即所有问题的解决目标都可以被描述成最大化累计信号的反馈奖励$$R_t$$.

序列决策的目标是选择一定的行为序列以最大化未来的总体奖励,此处注意是总体奖励,即有时会牺牲短期奖励来获得更多的长期奖励.

我们可以从个体和环境两个方面来描述强化学习问题.对于个体,在t时刻:
	1:有一个对于环境的观察评估$$O_t$$.
	2:做出一个行为$$A_t$$.
	3:从环境得到一个奖励信号$$R_{t+1}$$.

而环境则可以:
	1:接收个体的动作$$A_t$$.
	2:更新环境信息,同时使得个体可以得到下一个观测$$O_{t+1}$$.
	3:给个体一个奖励信号$$R_{t+1}$$.

下图为强化学习经典的一张图.它形象的表示个体与环境间的作用.

![](/img/in-post/deepRL_ch1/deepRL_ch1_1.jpg)

## 马尔科夫

为了后面采用马尔科夫决策过程,需引入状态这个概念,通俗的来讲状态是所有决定将来已有信息,即$$S_t = f(H_{t})$$.对于强化学习问题,状态分为环境状态和个体状态.环境状态是环境的私有呈现,包括环境用来决定一下个观测/奖励的所有数据,通常对个体并不完全可见,即使有时候对个体是完全可见的也有可能会掺杂一些无关的信息.个体状态是个体内部的体现,包括个体可以使用的,决定未来动作的所有信息.个体状态是强化学习算法可以利用的信息.若状态是马尔科夫的,则可以抛弃历史信息,仅需要t时刻状态就行.

当个体能够直接观测到环境状态时,即:个体对环境的观测 = 个体状态 = 环境状态,此时该问题为一个马尔科夫决策过程(Markov Decision Process, MDP).

而当个体只能观测部分环境时,即个体状态不等于环境状态,此时该问题是一个部分可观测的MDP.个体必须构建它自己的状态呈现形式,如记住完整的历史,Belief of environment state, RNN等方法,它们都是根据已有的数据,状态等信息尝试构建出当前个体的状态.

# 强化学习中的个体

## 强化学习个体的主要组成部分

#### 1:策略

策略是决定个体行为的机制,是从状态到行为的一个映射,可以是确定性的,也可以是不确定性的.

#### 2:价值函数

价值函数是一个未来奖励的预测,用来评价当前状态的好坏程度,用来指导个体制定不同的策略.一个价值函数是基于某一个特定策略的,不同策略下同一状态的价值并不相同.某一策略下的价值函数用下式表示:

$$ v_{\pi}(s) = E_{\pi}[R_{t+1} + \gamma R_{t+2} + \gamma^{2}R_{t+3} + \dots | S_{t} = s] $$

#### 3:模型

个体对环境的一个建模,它体现了个体是如何思考环境运行机制的,个体希望模型能模拟环境与个体的交互机制.模型并不是构建一个个体所必须的,很多强化学习算法中个体并不试图(依赖)构建一个模型.需要注意的是模型仅仅针对个体而言,环境的真实运行机制叫环境动力学(Dynamics of environment).

模型至少要解决两个问题,一个是状态转化概率,即预测下一个可能状态发生的概率:

$$ P_{ss^{'}}^{a} = P[S_{t+1} = s^{'} | S_{t} = s, A_{t} = a] $$

另一项工作是预测可能获得的即时奖励:

$$ R_{s}^{a} = E[R_{t+1} | S_{t} = s, A_{t} = a] $$

## 强化学习个体的分类

根据个体包含的"工具"可以把个体分为如下三类:

	1:仅基于价值函数的Value Based:在这样的个体中,有对状态的价值估计函数,但是没有直接的策略函数,策略函数由价值函数间接得到.
	2:仅直接基于策略的Policy Based: 这样的个体中行为直接由策略函数产生,个体并不维护一个对各状态价值的估计函数.
	3:演员-评判家形式Actor-Critic:个体既有价值函数,也有策略函数.两者相互结合解决问题.

此外根据个体在解决强化学习问题时是够建立一个对环境动力学的模型,将其分为两大类:

	1:不基于模型的个体: 这类个体并不试图了解环境如何工作，而仅聚焦于价值和/或策略函数。
	2:基于模型的个体：个体尝试建立一个描述环境运作过程的模型，以此来指导价值或策略函数的更新。

## 个体的学习与规划

个体的学习是个体在初始环境未知,不知道环境是如何工作的情况下,个体通过与环境进行交互,逐渐改善起行为策略.规划则指在个体对环境如何工作已知或近似时,个体并不与环境发生实际的交互,而是利用其构建的模型进行计算,在此基础上改善其行为策略.

一个常用的强化学习问题解决思路是先学习环境如何工作,即学习得到一个模型,然后利用这个模型进行规划.

## 预测和控制

在强化学习里，我们经常需要先解决关于预测（prediction）的问题，而后在此基础上解决关于控制（Control）的问题。

预测即给定一个策略，评价未来。可以看成是求解在给定策略下的价值函数（value function）的过程。How well will I(an agent) do if I(the agent) follow a specific policy? 而控制则是找到一个好的策略来最大化未来奖励.
