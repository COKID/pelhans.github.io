---
layout:     post
title:      "PRML学习笔记（四）"
subtitle:   "第四章 分类的线性模型"
date:       2018-10-29 00:15:18
author:     "Pelhans"
header-img: "img/prml.jpg"
header-mask: 0.3 
catalog:    true
tags:
    - PRML
---


> PRML 和 ESL 的学习基本上是学十得一。稳扎稳打再来一次

* TOC
{:toc}

# 分类的线性模型

在本章中我们考虑分类的线性模型。所谓分类线性模型，是指决策面是输入向量x的线性函数。在本章中讨论的算法同样适用于下面的情形：我们对输入变量进行一个固定的非线性变换，这个变换使用一个基函数向量$\phi(x)$。

## 4.1 判别函数

判别函数是一个以向量x为输入，把它分配到K个类别中的某一个类别(记为$C_{k}$)的函数。在本章中我们集中精力于线性判别函数，即那些决策面是超平面的判别函数。

接下来我们介绍三种学习先行判别函数的参数的方法：最小平方的方法、Fisher线性判别函数、感知器算法。

### 4.1.1 二分类

线性判别函数的最简单的形式是输入向量的线性函数，即：

$$ y(x) = w^{T}x + w_{0} $$

其中w被称为权向量，$w_{0}$被称为偏置，偏置的相反数有时被称为阈值。如果x是决策面内的一个点，那么$y(x)=0$ ，因此远点到决策面的垂直距离为：

$$ \frac{w^{T}x}{||w||} = - \frac{w_{0}}{||x||} $$

因此我们看到偏置参数$w_{0}$确定了决策面的位置。

### 4.1.2 多分类

K类判别函数由K个现行函数组成，形式为：

$$ y_{k}(x) = w_{k}^{T}x + w_{k0} $$

对于点x，如果对于所有的$j \neq k$都有 $y_{k}(x) > y_{j}(x)$，那么就把它分到$C_{k}$。于是类别 $C_{k}$ 和 $C_{j}$之间的决策面为 $y_{k}(x) = y_{j}(x) $，并且对应于一个(D-1)维超平面，形式为：

$$ (w_{k} - w_{j})^{T}x + (w_{k0} - w_{j0}) = 0 $$

这样的判别函数的决策区域总是单连通的，并且是凸的。 

### 4.1.3 用于分类的最小平方方法

每个类别$C_{k}$由我们自己的线性模型描述，我们可以很容易的把这些向量聚集在一起表示，即

$$ y(x) = \tilde{W}^{T}\tilde{x}  $$

其中$\tilde{W}$是一个矩阵，第k列由D+1维向量 $\tilde{w_{k}} = (w_{k0}, w_{k}^{T})^{T} $组成，$\tilde{x}$是对应的增广输入向量$(1, x^{T})^{T}$，它带有一个虚输入 $x_{0}=1$表示。这样，一个新的输入x被分配到输出$y_{k} = \tilde{w}^{T}\tilde{x}$ 最大的类别中。

我们现在通过最小化平方和误差函数来确定参数矩阵$\tilde{W}$。平方和误差函数可以写成：

$$ E_{D}(\tilde{W}) = \frac{1}{2}Tr\left\{ (\tilde{X}\tilde{W} - T)^{T}(\tilde{X}\tilde{W} - T)  \right\} $$

令上式关于$\tilde{W}$的导数等于零，整理可以得到$\tilde{W}$的解，形式为：

$$ \tilde{W} = (\tilde{X}^{T}\tilde{X})^{-1}\tilde{X}^{T}T = \tilde{X}^{\dag}T $$

其中$\tilde{X}^{\dag}$ 是矩阵$\tilde{X}$的伪逆矩阵，这样我们就得到了判别函数，形式为：

$$ y(x) = \tilde{W}^{T}\tilde{x} = T^{T}(\tilde{X}^{\dag})^{T}\tilde{x} $$

最小平方的方法对于判别函数的参数给出了精确地解析解。但是即使作为一个判别函数，它仍然有很严重的问题。最直观的问题是缺少对于离群点的鲁棒性。除此之外，由于最小平方法对应于高斯条件分布假设下的最大似然法，当数据点不满足该假设，如二值目标时其效果会很差。

### 4.1.4 Fisher线性判别函数

我们可以从维度降低的角度考察线性分类模型。首先考察二分类的情形。假设我们由一个D维输入向量x，然后使用某种变换$ y = w^{T}x $投影到一维。如果我们在y上设置一个阈值$w_{0}$，然后把$y \leq -w_{0}$的样本分为$C_{1}$类，其与样本分为$C_{2}$类，那么我们就得到了之前讨论的线性分类器。

为了避免数据在投影空间的相互重叠，我们可以调整权向量w使得两类数据样本的均值差最大化。即:

$$ max(m_{2} - m_{1}) = w^{T}(\mathbf{m_{2} - m_{1}}) $$

其中$m_{k} = w^{T}m_{k}$是来自$C_{k}$的投影数据的均值。但是通过增大w这个表达式可以任意大，因此我们将限制w为单位长度，即 $\sum_{i}w_{i}^{2} = 1$。使用拉格朗日乘数法来进行有限制条件的最大化问题的求解，我们可以发现$w \propto(m_{2} - m_{1})$。

然后对于上述方法，当我们投影到连接它们的均值的直线上时，就产生一定程度的重叠。如果概率分布的协方差矩阵与对角化矩阵差距较大就会出现这种问题，因此**Fisher提出的思想是最大化一个函数，这个函数能够让类均值的投影分开的较大，同时让每个类别内部的方差较小，从而最小化了类别的重叠。**

来自类别$C_{k}$的数据经过变换后的类内方差为：

$$ s_{k}^{2} = \sum_{n\in C_{k}}(y_{n} - m_{k})^{2} $$

Fisher准则为：

$$ J(w) = \frac{(m_{2} - m_{1})^{2}}{s_{1}^{2} - s_{2}^{2}} $$

重写上述公式，定义类间协方差矩阵$S_{B}$与类内协方差矩阵$S_{W}$：

$$ S_{B} = (m_{2} - m_{1})(m_{2} - m_{1})^{T} $$

$$ S_{W} = \sum_{n\in C_{1}}(x_{n} - m_{1})(x_{n} - m_{1})^{T} + \sum_{n\in C_{2}}(x_{n}-m_{2})(x_{n} - m_{2})^{T} $$

$$ J(w) = \frac{w^{T}S_{B}w}{w^{T}S_{W}w} $$

对于上式，我们发现J取最大值的条件为：

$$ (w^{T}S_{B}w)S_{W}w = (w^{T}S_{W}w)S_{B}w $$

忽略标量因子后得：

$$ w \propto S_{W}^{-1}(m_{2} - m_{1}) $$

上式被称为Fisher线性判别函数。虽然严格来说它并不是一个判别函数，但投影数据接下来可以用来构建判别函数(在投影坐标上选择一个阈值，分类即可)。

**对于二分类问题，Fisher准则可以看成最小平方的一个特列。**

对于多分类问题，根据Fisher思想，我们有很多准则选择方式，其中一种选择是：

$$ J(W) = Tr{s_{w}^{-1}s_{B}} $$

### 4.1.7 感知器算法

它对应于一个二分类模型，这个模型中，输入向量x首先使用一个固定的非线性变换得到一个特征向量$\phi(x)$，这个特征向量然后被用于构造一个一般的线性模型，形式为：

$$ y(x) = f(w^{T}\phi(x)) $$

其中非线性激活函数$f(*)$是一个阶梯函数，形式为：

$$ f(a) = \left\{
    \begin{aligned}
    +1, ~~~a\geq 0 \\
    -1, ~~~a\lt 0
    \end{aligned}
    \right.
    $$

我们采用感知器准则作为我们的误差函数：

$$ E_{P}(w) = -\sum_{n\in M}w^[T]\phi_{n}t_{n} $$

其中$ \phi_{n}$和M表示所有误分类模式的集合，总的误差函数的分段线性的。使用梯度下降算法可以优化权向量。

## 4.2 概率生成式模型


