---
layout:     post
title:      "PRML学习笔记（五）"
subtitle:   "第五章 神经网络"
date:       2018-11-02 00:15:18
author:     "Pelhans"
header-img: "img/prml.jpg"
header-mask: 0.3 
catalog:    true
tags:
    - PRML
---


> PRML 和 ESL 的学习基本上是学十得一。稳扎稳打再来一次

* TOC
{:toc}

# 神经网络

## 5.1 前馈神经网络

前馈神经网络即网络中不能存在有向圈，从而确保了输出是输入的确定函数。

假设有一个两层神经网络，即除输入层外有一个隐藏层和一个输出层。$\sigma$ 和 h 为激活函数 ，则整体的网络函数为：

$$ y_{k}(x, w) = \sigma\left( \sum_{j=1}^{M}w_{kj}^{2}h(\sum_{i=1}^{D}w_{ji}^{1}x_{i} + w_{j0}^{1}) + w_{k0}^{2} \right) $$

与感知器模型相比，一个重要的区别是神经网络在隐含单元中使用连续的  sigmoid 非线性函数，而感知器使用阶梯函数这一非线性函数。这意味着神经网络函数关于神经网络参数是可微的，这对于网络的训练很重要。

### 5.1.1 权空间对称性

前馈神经网络的一个性质是，对于不同的权向量w的选择，网络可能产生同样的从输入到输出的映射函数。该对称性不仅是双曲正切函数的特有性质，而是一大类激活函数都存在的性质。在许多情况下，

## 5.2 网络训练

我们要最小化误差函数：

$$ E(w) = \frac{1}{2}\sum_{n=1}^{N} || y(x_{n}, w) - t_{n} ||^{2} $$

### 回归问题

现在我们只考虑一元变量的情形，我们假定t服从高斯分布，均值与x相关，由神经网络的输出确定，即：

$$ p(t | x, w) = N(t | y(x,w), \beta^{-1}) $$

其中$\beta$是高斯噪声的精度。 激活函数采用恒等函数。在后面我们将该方法推广到更一般的情况。由此我们可以构造对应的似然函数：

$$ p(\mathbf{t} | \mathbf{X}, w, \beta) = \prod_{n=1}^{N}p(t_{n} | x_{n}, w, \beta) $$

取负对数就得到了误差函数：

$$ \frac{\beta}{2}\sum_{n=1}^{N}\{ y(x, w) - t_{n} \}^{2} - \frac{N}{2}\ln\beta + \frac{N}{2} \ln(2\pi) $$

这可以用来学习参数w 和 $\beta$。现在我们考虑最大似然法，首先确定w，最大化似然函数等价于最小化平方和误差函数：

$$ E(w) = \frac{1}{2}\sum_{n=1}^{N}\{y(x_{n}, w) - t_{n}  \}^{2} $$

通过最小化E(w)的方式可以得到w的最大似然姐。在实际应用中，由于 y(x,w)的非线性性质导致误差函数不是凸函数，因此实际应用中可能寻找的是似然函数的局部最大值，对应于误差函数的局部最小值。

找到$w_{ML}$后，$\beta$就可以通过最小化似然函数的负对数的方式的得到，结果为：

$$ \frac{1}{\beta_{ML}} = \frac{1}{N}\sum_{n=1}^{N}\{ y(x_{n}, w-{ML}) - t_{n} \}^{2} $$

在回归问题中，我们可以把神经网络看做具有一个恒等输出的激活函数的模型，即$y_{k} = a_{k} $，对应的平方和误差函数有如下性质：

$$ \frac{\partial E}{\partial a_{k}} = y_{k} - t_{k} $$

### 二分类情形

采用 sigmoid 激活函数，目标变量分布为伯努利分布，则负对数似然函数给出的误差函数就是一个交叉熵误差函数：

$$ E(w) = -\sum_{n=1}^{N}\{ t_{n}\ln y_{n} + (1-t_{n})\ln(1-y_{n}) \} $$

注意，此处没有与噪声精度$\beta$相类似的东西，因为我们假定目标值的标记都正确。对于分类问题，使用交叉熵损失函数而不是平方和误差损失函数会使得训练速度更快，同事提升泛化能力。

我们可以很容易的将其推广为多酚类问题，误差函数为：

$$ E(w) = -\sum_{n=1}^{N}\sum_{k=1}^{K}t_{nk}\ln y_{k}(x_{n}, w) $$ 

### 5.2.1 参数优化

由于误差E(w)是w的光滑连续函数，因此它的最小值出现在权空间中误差函数梯度等于领的位置上。梯度为零的点被称为住店，它可以进一步地被分为极小值点、极大值点和鞍点。

连续的非线性函数的最优化问题是一个被广泛研究的问题，有相当多的文献讨论如何高效地解决。大多数方法涉及到为权向量选择某个初始值$w_{0}$，然后在权空间进行一系列移动，形式为：

$$ w^{\tau + 1} = w^{\tau} + \Delta w^{\tau} $$

其中$\tau$表示迭代次数，不同的算法涉及到的权向量更新$\Delta w^{\tau}$的不同选择，许多算法会使用梯度信息。

### 5.2.2 局部二次近似

现在讨论误差函数的局部二次近似来寻找各种解决最优化问题的方法。

考虑E(w)在权空间某点出$\hat{w} $处的泰勒展开：

$$ E(w) \simeq E(\hat{w}) + (w - \hat{w})^{T}b + \frac{1}{2}(w - \hat{w})^{T}H(w - \hat{w}) $$

其中立方项和更高阶项已经被省略掉了。b被定义为E的梯度在$\hat{w}$处的值：

$$ b \equiv \nabla E|_{w=\hat{w}} $$

H为 Hessian矩阵，是E关于w的二阶导数在$\hat{w}$处的值。

当$w^{*}$为误差函数最小值点时，一阶导数项消失，于是有：

$$ E(w) \simeq = E(w^{*}) + \frac{1}{2}(w-w^{*})^{T}(w-w^{*}) $$

### 5.2.3 使用梯度信息

不适用梯度信息时，需要进行$O(W^{2})$次函数求值，每次求值都需要$O(W)$ 个步骤，因此时间复杂度为$ O(W^{3})$。使用梯度信息后整体的时间复杂度变为$ O(W^{2})$。

### 5.2.4 梯度下降最优化

最简单使用梯度信息的方法时：每次犬只更新都是在负梯度方向上的一次小的移动，即：

$$ w^{(\tau + 1)} = w^{(\tau)} - \eta\nabla E(w^{(\tau)}) $$

其中$\eta$时学习率。在每次更新后，梯度会使用新的权值向量重新计算，然后重复该过程。

对于批量最优化方法，存在更高效的方法，如共轭梯度法或拟牛顿法。与梯度下降法不同，这些算法 使得误差函数在每次迭代时总是减小的，除非权向量达到了局部的或者全局的最小值。

除此之外，我们还经常采用顺序梯度下降或随机梯度下降的方法，它使得权向量的每次更新只依赖于一个数据点：

$$ w^{(\tau+1)} = w^{(\tau)}- \eta\nabla E_{n}(w^{(\tau)}) $$

这个更新在数据集上循环重复进行。随机梯度下降的方法的另一个性质是，可以逃离局部极小值点，因为整个数据集的关于误差函数的驻点通常不会是每个数据点各自的驻点。

## 5.3 误差反向传播

关于反向传播算法此处只给出简单解释。反向传播的总结为：

1. 对网络的一个输入向量$x_{n}$，使用公式 $a_{j} = \sum_{j}w_{ji}z_{i} $和 $z_{j}=h(a_{j}) $进行正向传播，找到所有隐含单元和输出单元的激活。

2. 使用公式 $\delta_{j} = y_{k} - t_{k}$计算所有输出单元的$\delta$。

3. 使用公式 $\delta_{j} = h^{'}(a_{j})\sum_{k}w_{kj}\delta_{k} $$

4. 使用公式 $\frac{\partial E_{n}}{\partial w_{ji}} = \delta_{j}z_{i} $计算导数。

### 5.3.4 Jacobian 矩阵

使用反向传播计算 Jacobian矩阵的方法为：将输入空间中要寻找的 Jacobian 矩阵的点映射成一个输入向量，将这个输入向量作为网络的输入，使用通常的正向传播算法，得到网络的所有隐含单元和输出单元的激活。接下来，对于 Jacobian矩阵的每一行k(对应输出单元k)，使用递归关系

$$ \frac{\partial y_{k}}{\partial a_{j}} = \sum_{l}\frac{\partial y_{k}}{\partial a_{l}}\frac{\partial a_{l}}{\partial a_{j}} = h^{'}(a_{j})\sum_{l}w_{lj}\frac{\partial y_{k}}{\partial a_{l}}  $$

对网络中的所有隐含节点，反向传播开始于公式

$$\frac{\partial y_{k}}{\partial a_{l}} = \delta_{kl}^{'}(a_{l}) $$

$$\frac{\partial y_{k}}{\partial a_{l}} = \delta_{kl}y_{k} - y_{k}y_{l} $$

进行反向传播，最终使用

$$ J_{ji} = \sum_{j}w_{ji}\frac {\partial y_{k}}{\partial a_{j}} $$

进行对输入单元的反向传播。

通过反向传播，我们还可以计算 Hessian矩阵，具体方式可以参阅PRML。

## 5.5 神经网络的正则化

### 5.5.1 相容的高斯先验

相容性要求对网络的权值进行相应的线性变换使得输出不变两个网络应该是等价的，任何正则化项都应该与这个性质相容，否则模型就会倾向于选择某个解，而忽视某个等价的解。显然我们之前的权值衰减不满足这个要求。

满足该要求的一个正则化项为：

$$ \frac{\lambda_{1}}{2}\sum_{w\in W_{1}}w^{2} + \frac{\lambda_{2}}{2}\sum_{w\in W_{2}}w^{2} $$

其中$W_{1}$是第一层权值的集合，$W_{2}$表示第二层权值的集合，偏置未出现在求和式中。

### 5.5.2 早停止


